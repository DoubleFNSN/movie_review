{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `word2vec`\n",
    "Feature extraction is done via `word2vec`. `word2vec` model is trained from scratch with both labeled and unlabeled train data.\n",
    "\n",
    "Submission accuracy (average): `0.82724`  \n",
    "Submission accuracy (clustering): `0.80076`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download necessary NLTK files:\n",
    "* `stopwords`: Stopwords Corpus\n",
    "* `wordnet`: WordNet\n",
    "* `punkt`: Punkt Tokenizer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.donwload('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress warnings by BeautifulSoup:  \n",
    "```UserWarning: <string> looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.```  \n",
    "\n",
    "```UserWarning: <string> looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `word2vec` is unsupervised, we can use the unlabeled training data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 3), (50000, 2), (25000, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = 'data'\n",
    "df_labeled_train = pd.read_csv(join(src, 'labeledTrainData.tsv'), sep='\\t')\n",
    "df_unlabeled_train = pd.read_csv(join(src, 'unlabeledTrainData.tsv'), sep='\\t', quoting=3)\n",
    "df_test = pd.read_csv(join(src, 'testData.tsv'), sep='\\t')\n",
    "\n",
    "df_labeled_train.shape, df_unlabeled_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "We clean the data similarly to how it was done previously. However, we avoid removing numbers and stop words. We also leave each sentence as a list of words, instead of an entire string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_sentence(sentence, remove_stopwords):\n",
    "    removed_markup = BeautifulSoup(sentence, 'html.parser').get_text()\n",
    "    removed_punctuation = re.sub(r'[^a-zA-Z0-9]', ' ', removed_markup)\n",
    "    tokens = removed_punctuation.lower().split()\n",
    "    if remove_stopwords:\n",
    "        tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2vec` expects single sentences, each one as a list of words. In order to split a paragraph into sentences, we use NLTK's `punkt` tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review, remove_stopwords):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    return [clean_sentence(sentence, remove_stopwords) for sentence in raw_sentences if len(sentence) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:35<00:00, 694.49it/s]\n",
      "100%|██████████| 50000/50000 [01:13<00:00, 677.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in tqdm(df_labeled_train['review']):\n",
    "    sentences.extend(review_to_sentences(review, remove_stopwords=False))\n",
    "for review in tqdm(df_unlabeled_train['review']):\n",
    "    sentences.extend(review_to_sentences(review, remove_stopwords=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795872"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training `word2vec`\n",
    "**Parameters**  \n",
    "`num_features`: The dimension of each word vector  \n",
    "`min_word_count`: Words that occur fewer than this value across all training samples are ignored. Reasonable values are [10, 100].  \n",
    "`num_workers`: Parallel processing threads  \n",
    "`window`: Size of context window  \n",
    "`downsampling`: Amount of downsampling to use for frequent words. Google documentation recommends [1e-5, 1e-3].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 100\n",
    "num_workers = 4\n",
    "window = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 5s, sys: 559 ms, total: 4min 6s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "                          size=num_features,\n",
    "                          window=window,\n",
    "                          min_count=min_word_count,\n",
    "                          sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9469"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes the model more memory efficient, but only if not training any further\n",
    "model.init_sims(replace=True)\n",
    "model.save(f'model/features{num_features}_minword{min_word_count}_window{window}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match('france england germany berlin'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6231356859207153),\n",
       " ('lady', 0.6052083969116211),\n",
       " ('monk', 0.5642334222793579),\n",
       " ('lad', 0.5614923238754272),\n",
       " ('millionaire', 0.5309739112854004),\n",
       " ('guy', 0.5214076638221741),\n",
       " ('farmer', 0.5165209770202637),\n",
       " ('soldier', 0.5162621736526489),\n",
       " ('boxer', 0.5141708254814148),\n",
       " ('doctor', 0.5076218843460083)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing paragraphs: averaging\n",
    "For all words in each paragraph, we obtain their corresponding word vectors. The vector of the paragraph is then taken to be the average of all its word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_vec(sentence):\n",
    "    word_vecs = [model.wv[word] for word in sentence if word in model.wv.vocab]\n",
    "    \n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros([model.vector_size])\n",
    "    \n",
    "    feature_vec = np.stack(word_vecs)\n",
    "    return np.average(feature_vec, axis=0)\n",
    "\n",
    "def compute_review_vec(review):\n",
    "    feature_vec = np.stack([compute_sentence_vec(sentence) \n",
    "                            for sentence in review_to_sentences(review, remove_stopwords=True)\n",
    "                            if len(sentence) > 0])\n",
    "    return np.average(feature_vec, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:58<00:00, 423.92it/s]\n",
      "100%|██████████| 25000/25000 [00:56<00:00, 446.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((25000, 300), (25000, 300))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs = np.stack([compute_review_vec(review) for review in tqdm(df_labeled_train['review'])])\n",
    "test_vecs = np.stack([compute_review_vec(review) for review in tqdm(df_test['review'])])\n",
    "train_vecs.shape, test_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.1 s, sys: 9.31 ms, total: 41.1 s\n",
      "Wall time: 41.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(train_vecs, df_labeled_train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 644 ms, sys: 7.73 ms, total: 651 ms\n",
      "Wall time: 650 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = clf.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment\n",
       "0  12311_10          1\n",
       "1    8348_2          0\n",
       "2    5828_4          1\n",
       "3    7186_2          0\n",
       "4   12128_7          1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'id': df_test['id'], 'sentiment': pred})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission/word2vec_avg_randomforest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing paragraphs: clustering\n",
    "The words in the `word2vec` model vocabulary are first clustered. The vector for each paragraph will then be the sum of occurrences of each cluster, based on the words in that paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 40s, sys: 5min 27s, total: 15min 8s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_vectors = model.wv.vectors\n",
    "n_clusters = word_vectors.shape[0] // 5\n",
    "clf = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "idx = clf.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the words in 10 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['day' '75' 'block' 'chock']\n",
      "['imagining' 'weather' 'gangster' '85' 'protection' 'uncover' 'bradley'\n",
      " 'toe' 'recurring']\n",
      "['glorious' 'storm' 'abuse' 'indiana' 'mcdonald']\n",
      "['topless']\n",
      "['elegant' 'guts' 'ingredients' 'shed' 'owner' 'irish' 'warfare'\n",
      " 'realization' 'slater']\n",
      "['overly' 'shown' 'weller' 'aura' 'defending']\n",
      "['spelled' 'vinny' 'practices']\n",
      "['lyrics']\n",
      "['pursuit']\n",
      "['valuable' 'busey' 'disturbing' 'follow' 'holding' 'transfer'\n",
      " 'lackluster' 'resemble' 'concert' 'absent' 'falcon' 'overs']\n"
     ]
    }
   ],
   "source": [
    "model_vocab = np.array(list(model.wv.vocab.keys()))\n",
    "for i in range(10):\n",
    "    print(model_vocab[idx == i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary where the keys are the words in the `word2vec` vocabulary, and the values are the cluster indices of those words.  \n",
    "\n",
    "Look at a few sample words and their clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with 860\n",
      "all 1325\n",
      "this 1803\n",
      "stuff 543\n",
      "going 91\n",
      "down 430\n",
      "at 930\n"
     ]
    }
   ],
   "source": [
    "cluster_mapping = dict(zip(model_vocab, idx))\n",
    "for i, k in enumerate(cluster_mapping):\n",
    "    print(k, cluster_mapping[k])\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_centroid_vec(review):\n",
    "    centroid_counts = np.zeros([len(cluster_mapping)])\n",
    "    for sentence in review_to_sentences(review, remove_stopwords=True):\n",
    "        for word in sentence:\n",
    "            if word in cluster_mapping:\n",
    "                centroid_counts[cluster_mapping[word]] += 1\n",
    "    return centroid_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:36<00:00, 685.52it/s]\n",
      "100%|██████████| 25000/25000 [00:35<00:00, 698.35it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vecs = np.stack([review_centroid_vec(review) for review in tqdm(df_labeled_train['review'])])\n",
    "test_vecs = np.stack([review_centroid_vec(review) for review in tqdm(df_test['review'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 9469)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 351 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(train_vecs, df_labeled_train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.58 s, sys: 236 ms, total: 1.81 s\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred = clf.predict(test_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12311_10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8348_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5828_4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7186_2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12128_7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment\n",
       "0  12311_10          1\n",
       "1    8348_2          0\n",
       "2    5828_4          1\n",
       "3    7186_2          1\n",
       "4   12128_7          1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'id': df_test['id'], 'sentiment': pred})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission/word2vec_cluster_randomforest.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
